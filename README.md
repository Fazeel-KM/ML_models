# OpenSearch Anomaly Pipeline

End-to-end workflow for downloading OpenSearch indices, cleaning API event data, engineering features, training anomaly detectors, and exporting anomalies. The orchestrator is `main_script.py`, which stitches together the helper modules in this repo.

## Project Layout

- `main_script.py` – CLI pipeline entry-point (download ➜ clean ➜ feature engineer ➜ train/evaluate)
- `download_indexWise.py` – utilities for listing and downloading indices from OpenSearch
- `filter.py` – JSON cleaning and descriptive statistics for raw events
- `feature_engineering.py` – feature construction, metadata export, and encoder artifacts
- `model.py` – model training (Isolation Forest, LOF, One-Class SVM)
- `evaluate_models.py` – model evaluation and anomaly export
- `data/` – cached raw, cleaned, features, and artifact folders (large; see GitHub prep notes)
- `models/` – saved model pickles
- `requirements.txt` – Python dependencies

## Prerequisites

- Python 3.10 or newer
- Ability to reach your OpenSearch cluster (defaults to `http://3.27.173.116:9200`; override with `--es-url`)
- Enough disk space for raw/cleaned data, engineered features, and model artifacts

Recommended environment setup:

```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
pip install -r requirements.txt
```

## Data & Artifact Directories

`main_script.py` creates a hierarchy under `data/` (configurable via `--data-dir`):

- `raw/` – downloaded JSON dumps of indices
- `clean/` – cleaned JSON generated by `filter.py`
- `features/` – engineered feature CSVs + metadata
- `artifacts/` – scaler pickles, validation reports, exported anomalies

Trained models are saved under `models/` (configurable via `--models-dir`).

## Using `main_script.py`

Run the pipeline with either interactive prompts or CLI flags.

### Key Arguments

| Flag | Description |
| --- | --- |
| `--mode {train,test}` | Pipeline mode. If omitted and running interactively, you will be prompted (defaults to `train` when piped). |
| `--indices` / `--index` | Comma-separated list or specific index name(s) to download. |
| `--index-serials` / `--index-serial` | Select indices by serial number from the interactive listing. |
| `--combine` | Merge multiple cleaned datasets before feature engineering (train mode only). |
| `--contamination` | Expected outlier rate for models (default `0.01`). |
| `--evaluate` | After training, immediately run evaluation and export anomalies. |
| `--eval-model-tag`, `--eval-dataset` | Specify which trained bundle/dataset to evaluate. |
| `--force-download` | Re-download raw JSON even if it already exists locally. |
| `--quiet` | Reduce logging chatter. |

See `python main_script.py --help` for a full list.

### Train Mode

1. **Select indices**  
   Provide names or serials via flags, or choose interactively.
2. **Clean & engineer features**  
   Cleansed files are stored in `data/clean/`; features, metadata, and encoders land in `data/features/`.
3. **Train models**  
   Isolation Forest, LOF, and One-Class SVM models plus scalers are saved under `models/` and `data/artifacts/`.
4. **Optional evaluation**  
   Add `--evaluate` to run `evaluate_models.py` immediately. Anomalies are exported to `data/artifacts/model_<tag>_anomalies.json`.

Example (combine two indices and evaluate):

```bash
python main_script.py ^
  --mode train ^
  --indices events-20251024,events-20251026 ^
  --combine ^
  --contamination 0.02 ^
  --evaluate
```

### Test Mode

Use when you already have trained models and want to score a dataset.

```bash
python main_script.py ^
  --mode test ^
  --index events-20251114 ^
  --eval-model-tag combined_events-20251024_events-20251026 ^
  --eval-dataset data/features/combined_events-20251024_events-20251026_features.csv
```

If arguments are omitted and you are in an interactive terminal, you will be prompted to choose the model bundle and feature dataset. The evaluation step produces summary statistics and, when metadata or cleaned records are available, writes detailed anomalies to `data/artifacts/model_<tag>_anomalies.json`.

## Manual Utilities

- `python download_indexWise.py --index my-index --output-dir data/raw --non-interactive`  
  Download a specific index to JSON.
- `python feature_engineering.py --input data/clean/events-20251114_cleaned.json --output-dir data/features`  
  Re-run feature engineering on an existing cleaned dataset.
- `python evaluate_models.py --models-dir models --artifacts-dir data/artifacts --dataset data/features/events-20251114_features.csv --model-tag events-20251114 --cleaned-path data/clean/events-20251114_cleaned.json --metadata-path data/features/events-20251114_metadata.csv`  
  Evaluate outside the full pipeline or tweak consensus thresholds with `--consensus-threshold`.

## Preparing for GitHub

Large data caches and binary artifacts can bloat the repository. Before pushing:

1. **Clean or ignore generated assets**  
   Add a `.gitignore` (or extend your existing one) with entries such as:
   ```
   data/raw/
   data/clean/
   data/features/
   data/artifacts/
   models/
   __pycache__/
   *.pkl
   *.json
   ```
   Keep a small sample dataset if needed for demonstration.
2. **Document credentials**  
   Avoid committing sensitive OpenSearch URLs or API tokens. Consider using environment variables for `--es-url`.
3. **Reproduce results**  
   Commit only lightweight artifacts (e.g., a sample combined anomalies file) or rely on the pipeline to regenerate outputs.
4. **Add project badges or screenshots (optional)**  
   Include visuals or metrics once the repo is public.

## Next Steps

- Create issues or discussions outlining future improvements (automated scheduling, model comparison dashboards, etc.).
- Consider wrapping the pipeline in a task runner (Makefile, Invoke, or GitHub Actions) for repeatability.
- Add unit or integration tests for data processing steps when the dataset stabilises.

With the README in place and the repo cleaned, you are ready to publish the project on GitHub.

